[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/ymop5HUw)
# CMPSC 310 Activity 15

## Deadline: April 12 by 9:50am

## Assignment

 For this activity follow [Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer).

## Submission

Submit completed Colab notebook showing generated output.

Andrew: I tried running the Colab twice and my project crashed in the training after hours of training. 

Data: The data in this sense is natural language as we understand it. The computer could have a difficult time making sense of our way of speaking, as the way we derive meaning isn't easily injected into an algorithm that can just churn out those results. We have a deep, intricate understanding of verbigae, possession, the items that these objects act on (Direct Objects).

Preprocessing & transformer: Using the transformers, the algorithm is able to make parallel relations between points in the data, making it more "efficient" in a sense than CNN or RNN. RNNs and CNNs can't run things sequentially like transformers can, making them ideal for natural language processing.
